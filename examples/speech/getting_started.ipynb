{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from IPython.display import display\n",
    "import numpy as np \n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## Set seed\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "from ferret import SpeechBenchmark, AOPC_Comprehensiveness_Evaluation_Speech, AOPC_Sufficiency_Evaluation_Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'FSC'\n",
    "data_dir = f'{str(Path.home())}/data/speech/fluent_speech_commands_dataset'\n",
    "\n",
    "# We read the test data of FSC dataset\n",
    "df = pd.read_csv(f\"{data_dir}/data/test_data.csv\")\n",
    "df[\"path\"] = df[\"path\"].apply(lambda x: os.path.join(data_dir, x))\n",
    "\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_str = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device_str)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load model\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    \"superb/wav2vec2-base-superb-ic\"\n",
    ")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "    \"superb/wav2vec2-base-superb-ic\"\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech-XAI: the `SpeechBenchmark` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate benchmark class\n",
    "benchmark = SpeechBenchmark(model, feature_extractor, device=device_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example\n",
    "# 'transcription': 'Turn up the bedroom heat.'\n",
    "# 'action': 'increase'\n",
    "# 'object': 'heat'\n",
    "# 'location': 'bedroom'\n",
    "\n",
    "idx = 136\n",
    "audio_path = dataset[idx]['path']\n",
    "\n",
    "audio = AudioSegment.from_wav(audio_path)\n",
    "display(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain word importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = benchmark.explain(\n",
    "    audio_path_or_array=audio_path, \n",
    "    methodology='LOO'\n",
    ")\n",
    "\n",
    "display(benchmark.show_table(explanation, decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = benchmark.explain(\n",
    "    audio_path_or_array=audio_path, \n",
    "    methodology='LIME'\n",
    ")\n",
    "\n",
    "display(benchmark.show_table(explanation, decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aopc_compr = AOPC_Comprehensiveness_Evaluation_Speech(benchmark.model_helper)\n",
    "evaluation_output_c = aopc_compr.compute_evaluation(explanation)\n",
    "\n",
    "aopc_suff = AOPC_Sufficiency_Evaluation_Speech(benchmark.model_helper)\n",
    "evaluation_output_s = aopc_suff.compute_evaluation(explanation)\n",
    "\n",
    "evaluation_output_c, evaluation_output_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with transcriptions explicitly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Ferret` offers an interface with ASR (automatic speech recognition) models from [`WhisperX`](https://github.com/m-bain/whisperX) in the form of the `transcribe_audio` function. This is called from within `Ferret` and there's no need to access it explicitly. Nevertheless, should the need arise, here's how to generate the word-level transcript (with time alignments for the audio part) used internally by the `SpeechBenchmark.evaluate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ferret.explainers.explanation_speech.utils_removal import transcribe_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, words_trascript = transcribe_audio(\n",
    "    audio_path=audio_path,\n",
    "    device=device.type,\n",
    "    batch_size=2,\n",
    "    compute_type=\"float32\",\n",
    "    language='en'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = benchmark.explain(\n",
    "    audio_path=audio_path, \n",
    "    methodology='LOO',\n",
    "    # Transcripts are passed explicitly.\n",
    "    words_trascript=words_trascript\n",
    ")\n",
    "\n",
    "display(benchmark.show_table(explanation, decimals=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain paralinguistic impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_table = benchmark.explain(\n",
    "    audio_path=audio_path,\n",
    "    methodology='perturb_paraling',\n",
    ")\n",
    "display(benchmark.show_table(explain_table, decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbation_types = ['time stretching', 'pitch shifting', 'reverberation', 'noise']\n",
    "variations_table = benchmark.explain_variations(\n",
    "    audio_path=audio_path,\n",
    "    perturbation_types=perturbation_types\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variations_table_plot = {k:variations_table[k] for k in variations_table if k in ['time stretching', 'pitch shifting', 'noise']}\n",
    "fig = benchmark.plot_variations(variations_table_plot, show_diff = True, figsize=(4.6, 4.2));\n",
    "# fig.savefig(f'example_{dataset_name}_context.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "SUPERB - IC Task (FSC).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
